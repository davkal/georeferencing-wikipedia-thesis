%************************************************
\chapter{Apparatus}\label{ch:apparatus}
%************************************************

This chapter describes data sources to get Wikipedia content like articles and revision history as well as tools to retrieve and analyze those.

\section{Data sources}

For an automated analysis browsing the user interface of Wikipedia's website is not really feasible.
The bulk of Wikipedia's content like articles, revisions, discussions is stored on its database servers.
Unfortunately, these databases are not directly accessible over the Internet.
The Wikimedia Foundation, however, makes a lot of the data available in the form of database dumps or through an application programming interface (API).

\subsection{Database dumps}

Monthly database snapshots of all wikis run by the Wikimedia Foundation, including Wikipedia,  are publicly available\footurl{http://dumps.wikimedia.org}{2011}{12}{11} as database dump files in the XML file format.
For each of the wikis a variety of dumps is available that include all articles and, optionally, their revision history, all categories, interwiki-links, etc.
Despite this openness, some database tables are not publicly available.
The dump files of the \emph{users} and the \emph{watchlist} tables are kept private.

The dump files can be quite large, e.g. a compressed dump of all articles of the English Wikipedia in their current revision has a size 7.3 GB.\footnote{The uncompressed size is 31.0 GB, see \weburl{http://en.wikipedia.org/wiki/Wikipedia:Database_download}{2011}{12}{11}}
This huge size makes processing them rather slow.\footnote{The project WikiHadoop addresses this problem by offering a stream task format to be used in Hadoop (MapReduce) infrastructure, see \weburl{https://github.com/whym/wikihadoop}{2011}{12}{11}.}
When analyzing only a single article or a category articles, the MediaWiki API can deliver the same information contained in the dumps in a much more targeted manner. 

\subsection{MediaWiki API}

Wikipedia runs on the open source software MediaWiki.
This PHP-based wiki package offers a well documented API which can be used by other programs to remotely use the wiki's features such as changing content and restoring revisions.
For analysis of articles, the API offers queries directed at a variety of article properties, e.g. revisions, categories and links.
Similar to MediaWiki's Special:Export page\footnote{The page \weburl{https://en.wikipedia.org/wiki/Special:Export}{2011}{12}{11} allows for exporting of articles from the English Wikipedia.}, the API also offers an article export that includes all revisions.

The following queries will be important for this thesis:

\begin{todos}
    \item check if a user is a bot 
    \item all articles that are member of a category
    \item revisions, includes user names
\end{todos}

\subsection{Toolserver}\label{sub:toolserver}

The Germany based Wikimedia Deutschland e.V. runs Toolserver\footurl{http://toolserver.org}{2011}{12}{11}, a platform for software tools that can access a continuously updated copy of Wikipedia's databases. 
Among these replicated databases is the English Wikipedia and other major language editions.
However, the deployment self-made software scripts is restricted and requires an account on Wikimedia's Toolserver.

Some scripts that are already deployed can be accessed freely, allowing them to be reused.
One of these was developed by SoNet\footurl{http://sonetlab.fbk.eu/}{2011}{12}{12}, a social networking research group.
It offers an API\footnote{The API is documented here: \weburl{https://github.com/volpino/toolserver-scripts/tree/master/php}{2011}{12}{12}} to get simple article statistics like article ID, text length as well as complex data structures like a list of unique editors including their gender if they are registered users and chose to reveal their gender in their Wikipedia account.\footnote{Try \weburl{http://toolserver.org/~sonet/api_gender.php?article=Egypt&lang=en}{2011}{12}{11} to get a list of all registered users who edited the article \emph{Egypt} of the English Wikipedia.}


\subsection{Third-party sources}

Some research projects can be reused as data sources.
Depending on the project's goal, preprocessed statistics can become available:

\begin{description}
\item[Article traffic] Wikipedia user Henrik\footurl{http://en.wikipedia.org/wiki/User:Henrik}{2011}{12}{12} provides a website that processes Wikipedia's log files\footnote{These are available at \weburl{http://dumps.wikimedia.org/other/pagecounts-raw/}{2011}{12}{12}} to calculate the number page views per article for a given time.
These statistics can be viewed through a browser\footnote{E.g. \weburl{http://stats.grok.se/en/201105/2011_Egyptian_Revolution}{2011}{12}{12}} or via an API \footnote{E.g. \weburl{http://stats.grok.se/json/en/201105/2011_Egyptian_Revolution}{2011}{12}{12}}. 
\end{description}


\section{Available Tools}

In the open source community a wide range of software tools are available.
A simple search for ``Wikipedia'' on GitHub\footurl{https://github.com/search?q=wikipedia&type=Repositories}{2011}{12}{12}, a source code exchange platform, shows a host of small software projects.
These come in different programming languages and different feature sets and usually help in downloading articles in batches and extract data from big dump files. 
Developed by vigilantes and researchers alike, these programs facilitate both data retrieval and processing.

\subsection{Toolkits}

A group of openly available software packages qualify as swiss-army knifes for processing dumps and sending requests to the API:

\begin{description}
\item[wikidump] Tools to get
\end{description}

\subsection{Analysis projects}

This section describes useful toolkits to analyze articles and their revision history.

\begin{description}
\item[WikiPride] Python web-application\footurl{https://github.com/declerambaul/WikiPride}{2011}{12}{11}  to visualize contributions of groups of editors that registered in the same month.\footnote{Project website: \weburl{http://meta.wikimedia.org/wiki/Research:WikiPride}{2011}{12}{11}}
\item[Wiki Trip] JavaScript application\footurl{https://github.com/volpino/wikipedia-timeline}{2011}{12}{11} that uses the Wikipedia API as well as a Toolserver backend, see \nameref{sub:toolserver}, to visualize the evolution of a single article over time including: anonymous vs. registered contributors, male vs. female registered users, anonymous edits by country.\footnote{Live demo: \weburl{http://sonetlab.fbk.eu/wikitrip/}{2011}{12}{11}} 
\item[] 
\end{description}


\begin{todos}
\item Revert analyzer and framework to map a processing function on dumps: \url{https://bitbucket.org/halfak/wikimedia-utilities/}
\end{todos}

\section{Collective Authorship}

\todo{Introduce types of authors (roles) as well as methods to determine contribution/attribution}

\begin{todos}
    \item Autoren
    \item Bots
    \item Wer überlebt?
    \item Algorithmen, welche Unterschiede?
\end{todos}

\subsection{Relevant Edits}

\todo{Are all edits relevant? Edit wars? Bots?}



\section{Georeferences}

\begin{todos}
    \item registered vs. unregistered vs. bots vs. admins
    \item incorporate key findings of \cite{hardy2011volunteered} as laid out in chapter \ref{sec:georeference}
    \item IPs of unregistered users: Geo lookup
    \item Autoren-Profile: Information Extraction
    \item Geographische Zuordnung vom user profile
\end{todos}

Zur Bestimmung der Herkunft eines Autors bietet Wikipedia zwei direkte Ansätze: 
Für jeden Beitrag eines nicht registrierten Benutzers wird die IP-Adresse gespeichert, über die er Zugang zum Internet erlangt hat. 
Der zweite Ansatz betrifft die registrierten Benutzer.
Ihre IP-Adressen sind maskiert und nicht öffentlich zugänglich.
\footnote{Das WikiWatcher-Teilprojekt \emph{Poor Man's Check User} erlaubt eine Auflösung des Benutzernamens in eine IP-Adresse, wenn dieser Nutzer in der Vergangenheit beim Ändern eines Artikels das Session-Limit überschritten hatte. Inzwischen wurde diese Sicherheitslücke in der WikiMedia-Software jedoch behoben. \url{http://wikiwatcher.virgil.gr/pmcu}}
\footnote{Eine kleine, von der Wikipedia-Community gewählte Nutzerschaft mit der Berechtigung \emph{checkuser} kann die Adressen demaskieren.}
Die registrierten Nutzer können jedoch auf ihrer \emph{user page} Informationen über ihre Person entweder als Freitext oder strukturiert in \emph{user boxes} veröffentlichen.
Letztere sind definierte Einheiten mit denen der Nutzer persönliche Eigenschaften wie Herkunftsland, gesprochene Sprachen oder wissenschaftliche Interessen kodifizieren kann.
Zusammen decken beide Ansätze jedoch nur einen Teil der Beiträge schreibenden Nutzerschaft ab.


\subsection{IP Look-up}

\begin{todos}
    \item Services
    \item Accuracy
    \item Active prevention by proxies and anonymizers: 
    \\ \fullcite{muir2006internet} 
    \\ \fullcite{muir2009internet}
    \\ \fullcite{duckham2005formal}
\end{todos}

Mit frei verfügbaren\footnote{Die vorgestellten Dienste haben ein tägliches Kontingent an Anfragen. Hilfstechniken wie Caching können diese Einschränkungen jedoch mindern.} Online-Diensten wie \term{Quova}\footnote{\url{http://developer.quova.com}} oder \term{geoplugin}\footnote{\url{http://www.geoplugin.com/webservices}} lässt sich für einen Großteil der IPs daraufhin das Herkunftsland bestimmen.

Im Bezug auf die Herkunft sind sowohl das Land als auch die Geo-Koordinaten interessant.  
Basierend auf der Versionsgeschichte würde für nicht registrierte Benutzer eine Gewinnung von Daten dann beispielsweise folgende Schritte durchlaufen:

\begin{quotation}
IP \RA Geolocation-Dienst \RA Koordinaten und Land
\end{quotation}


\subsection{Information Extraction}

\begin{todos}
\item Automatic annotation of entities: \url{http://wikipedia-miner.cms.waikato.ac.nz/demos/annotate/}, also has services for categories. Alternative: \url{http://tagme.di.unipi.it/}
    \item IE approach with Machine Learning \fullcite{xiao2004information}
    \item unsupervised IE: \fullcite{etzioni2005unsupervised}
    \item if city is mentioned, determine country (needs disambiguation, e.g. Berlin)
    \item coordinates are optional?
\end{todos}


\subsection{Geographic Profiling}

\begin{todos}
    \item \fullcite{lieberman2009you}
    \item \fullcite{hecht2010localness}
    \item from other fields such as criminal research: \\ \fullcite{snook2005complexity}
    \item feasibility, maybe just as enhancer
\end{todos}


\subsection{Consolidation}

\begin{todos}
    \item settle for a resolution
    \item some examples on accuracy for different countries
    \item clustering of origins: areas of influence
\end{todos}



\section{Visualization}

\subsection{Maps}

\begin{description}
\item[OpenLayers] JavaScript library\footurl{http://www.openlayers.org/}{2011}{12}{11} to render map data in the browser.
\end{description}

\begin{todos}
    \item Darstellung der geographischen Analyse
    \item per Wort, Satz, Artikel, Wort
\end{todos}

Auf Basis der strukturierten Daten in Form von Artikeln, Sätzen, Ländern, Koordinaten und Sprachen sollen nun Visualisierungen gefunden werden, welche die Fülle an Informationen zugänglich machen.
Mögliche Visualisierungen wären etwa:

\begin{labeling}{V2}
\item[V1] Revisionshistogramm à la Google Finance 
\item[V2] \emph{Heatmap} einer Landkarte mit Ursprüngen der Revisionen 
\item[V3] Netzwerkgrafik, die Metriken desselben Artikels in verschiedenen Sprachvarianten anzeigt
\item[V4] Dynamisches Blasendiagramm\footnote{\url{http://en.wikipedia.org/wiki/Motion_chart}} über die Entwicklung unterschiedlicher Sprachvarianten
\item[V5] \emph{Heatmap} des Artikels mit Stellen höchster Aktivität 
\item[V6] Landeskürzel für eine gegebene Textstelle
\item[V7] Edit wars on map, linking two or more places
\end{labeling}


\subsection{Goals}

\begin{todos}
    \item \fullcite{kjellin2010evaluating}
    \item Identify. Characteristics of an object.
    \item Locate. Absolute or relative position.
    \item Distinguish. Recognize as the same or different.
    \item Categorize. Classify according to some property (e.g., color, position, or shape).
    \item Cluster. Group same or related objects together.
    \item Distribution. Describe the overall pattern.
    \item Rank. Order objects of like types.
    \item Compare. Evaluate different objects with each other.
    \item Associate. Join in a relationship.
    \item Correlate. A direct connection.
\end{todos}


\subsection{Design}


\section{Data Model and System Overview}

\begin{todos}
    \item fetch article
    \item get revision history
    \item determine contributions
    \item transform to word attribution
    \item attach georeference 
\end{todos}

\section{Analysis}

\begin{todos}
    \item article usage (page views) vs. article production (edits)
    \item 
    \item 
    \item
    \item 
\end{todos}


