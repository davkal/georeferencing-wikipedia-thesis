%************************************************
\chapter{Apparatus}\label{ch:apparatus}
%************************************************

This chapter describes data sources to get Wikipedia content like articles and revision history as well as tools to retrieve and analyze those.

\todo{Intro aspects of apparatus}

\section{Data sources}

For an automated analysis, simply browsing Wikipedia's website is not really feasible. 
The bulk of Wikipedia's content like articles, revisions, discussions is stored on its database servers.
Unfortunately, these databases are not directly accessible over the Internet.
The Wikimedia Foundation, however, makes a lot of the data available in the form of database dumps or through an application programming interface (API).

\subsection{Wikipedia website}

For a complete article analysis, navigating the website can be tedious as one would have to click through a complete revision history and parse the page's source which is formatted using the HyperText Markup Language (HTML).
However, individual pages contain data that is static and can be used throughout the analysis process.
This makes it worth writing a specific parser for a technique known as \emph{screen scraping} to extract the information.
On a high level, it involves the following steps:

\begin{enumerate}
  \item Looking at the HTML source of the page and identifying how the HTML tags and attributes that are used to structure the information.
  \item Writing a parser that addresses the identifying tags and thereby tokenizes the data.
  \item Converting the found tokens into an output format, e.g. JSON.
\end{enumerate}

For a simple HTML table, a parser can be written in a few lines of code.
Using this technique, the following static information was gathered:

\begin{description}
\item[bots] A list of bots was built based on the Wikipedia page \emph{List of bots by number of edits}\footurl{http://en.wikipedia.org/wiki/Wikipedia:List_of_bots_by_number_of_edits}{2012}{01}{24}. 
This list is used to distinguish bots from real authors as contributions done by bots are excluded from the analysis.
There are unregistered bots, however, that appear not in the list.
For a lack of automated distinction, these are counted as normal authors.\footnote{A simple heuristic employed by other software to analyze MediaWiki content is treating all contributors whose username contains or whose comments start with ``bot'' as a bot, e.g. pymwdat, see \weburl{http://code.google.com/p/pymwdat/source/browse/trunk/toolkit.py?spec=svn13&r=13}{2012}{01}{24}. This has a potential for false positives and is not used.}
\item[countries] A list of countries was extracted from the article \emph{ISO\_3166-1}\footurl{http://en.wikipedia.org/wiki/ISO_3166-1}{2012}{01}{2}. 
It provides a list of standardized country names that is also respected by Wikipedia's authors when referring to a country by name.
In a second pass, the Wikipedia article of each country was parsed for coordinates.\footnote{For some countries, coordinates were not present on the page, e.g. \weburl{http://en.wikipedia.org/wiki/Australia}{2012}{01}{24}. In that case, they were manually added by using that country capital's coordinates. For a discussion on geographic resolution, see \ref{sub:resolution}.}
\end{description}

Making both of these sets static was a design decision recognizing the trade-off between having them in memory and querying for each article.

\subsection{Database dumps}

Monthly database snapshots of all wikis run by the Wikimedia Foundation, including Wikipedia,  are publicly available\footurl{http://dumps.wikimedia.org}{2011}{12}{11} as database dump files in the XML file format.
For each of the wikis a variety of dumps is available that include all articles and, optionally, their revision history, all categories, interwiki-links, etc.
Despite this openness, some database tables are not publicly available.
The dump files of the database tables \emph{users} and the \emph{watchlist} are kept private.

The dump files can be quite large, e.g. a compressed dump of all articles of the English Wikipedia in their current revision has a size 7.3 GB.\footnote{The uncompressed size is 31.0 GB, see \weburl{http://en.wikipedia.org/wiki/Wikipedia:Database_download}{2011}{12}{11}}
This huge size makes processing them rather slow.\footnote{The project WikiHadoop addresses this problem by offering a stream task format to be used in Hadoop (MapReduce) infrastructure, see \weburl{https://github.com/whym/wikihadoop}{2011}{12}{11}.}
When analyzing only a single article or a category articles, the MediaWiki API can deliver the same information contained in the dumps in a much more targeted manner. 

\subsection{MediaWiki API}

Wikipedia runs on the open source software MediaWiki.
This PHP-based wiki package offers a well documented API which can be used by other programs to remotely use the wiki's features such as changing content and restoring revisions.
For analysis of articles, the API offers queries directed at a variety of article properties, e.g. revisions, categories and links.
Similar to MediaWiki's Special:Export page\footnote{The page \weburl{https://en.wikipedia.org/wiki/Special:Export}{2011}{12}{11} allows for exporting of articles from the English Wikipedia.}, the API also offers an article export that includes all revisions.

\todo{Sandbox, Example call}

The following queries will be important for this thesis:

\begin{todos}
    \item query info (pageid info, wikitext, langlinks, ...)
    \item parse (HTML version of wikitext)
    \item parse (HTML user pages)
    \item open search (autocomplete search)
    \item revisions (all revisions, first revisions of all languages)
    \item category members
    \item template embedders
\end{todos}

\todo{Category and template embedders will be used to get groups of articles}

\subsection{Toolserver}\label{sub:toolserver}

The Germany based Wikimedia Deutschland e.V. runs Toolserver\footurl{http://toolserver.org}{2011}{12}{11}, a platform for software tools that can access a continuously updated copy of Wikipedia's databases. 
Among these replicated databases is the English Wikipedia and other major language editions.
However, the deployment of self-made software scripts is restricted and requires an account on Wikimedia's Toolserver.\footnote{I applied for a Toolserver account outlining my necessary database queries, usage profile as well as my affiliation with the Freie Universit\"at Berlin. The application was submitted on 2011-12-21 and has not been processed yet (2012-01-23).}

Some scripts that are already deployed can be accessed freely, allowing them to be reused.
One of these was developed by SoNet\footurl{http://sonetlab.fbk.eu/}{2011}{12}{12}, a social networking research group.
\todo{link to wikitrip} 
It offers an API\footnote{The API is documented here: \weburl{https://github.com/volpino/toolserver-scripts/tree/master/php}{2011}{12}{12}} to get simple article statistics like article ID, text length as well as complex data structures like a list of unique editors including their gender if they are registered users and chose to reveal their gender in their Wikipedia account.\footnote{Try \weburl{http://toolserver.org/~sonet/api_gender.php?article=Egypt&lang=en}{2011}{12}{11} to get a list of all registered users who edited the article \emph{Egypt} of the English Wikipedia.}


\subsection{Third-party sources/Web services}

Some research projects can be reused as data sources.
Depending on the project's goal, preprocessed statistics can become available:

\begin{description}
\item[Article traffic] Wikipedia user Henrik\footurl{http://en.wikipedia.org/wiki/User:Henrik}{2011}{12}{12} provides a web service that processes Wikipedia's log files\footnote{These are available at \weburl{http://dumps.wikimedia.org/other/pagecounts-raw/}{2011}{12}{12}} to calculate the number page views per article for a given time.
These statistics can be viewed through a browser\footnote{E.g. \weburl{http://stats.grok.se/en/201105/2011_Egyptian_Revolution}{2011}{12}{12}} or via an API \footnote{E.g. \weburl{http://stats.grok.se/json/en/201105/2011_Egyptian_Revolution}{2011}{12}{12}}. 
\item[CatScan] This web service offered by Toolserver administrator Duesentrieb\footurl{http://meta.wikimedia.org/wiki/User:Duesentrieb}{2011}{12}{13} finds articles that belong to a given category and its sub-categories\footnote{See \nameref{sub:categories} on why this is non-trivial.}.
It also offers to limit the search to an intersection of categories, e.g. German politicians who are also physicists\footurl{https://toolserver.org/~daniel/WikiSense/CategoryIntersect.php?wikilang=de&wikifam=.wikipedia.org&basecat=Politiker+(Deutschland)&basedeep=3&mode=cs&tagcat=Physiker&tagdeep=2}{2011}{12}{13}.
The results are presented in the browser or can be downloaded as a file containing comma-separated values (CSV format). 
\item[Poor man's checkuser] \todo{Username to IP resolution, link to georeferencing}
\item[Quova] \todo{IP to location resolution, link to georeferencing}
\item[WikiTrust] \todo{annotations}
\end{description}


\section{Available Tools}

In the open source community a wide range of software tools are available.
A simple search for ``Wikipedia'' on GitHub\footurl{https://github.com/search?q=wikipedia&type=Repositories}{2011}{12}{12}, a source code exchange platform, shows a host of small software projects.
These come in different programming languages and different feature sets and usually help in downloading articles in batches and extract data from big dump files. 
Developed by vigilantes and researchers alike, these programs facilitate both data retrieval and processing.

\subsection{Toolkits}\label{sub:toolkits}

A group of openly available software packages qualify as swiss-army knifes for processing dumps and sending requests to the API:

\begin{description}
\item[pywikipedia] \todo{the mother of all python wp toolkits, classes for all mediawiki entities like page, user, revision}
\item[pymwdat] \todo{relies on pywikipedia, dump file analysis, revisions, revert detection, filtering, helps in understanding}
\item[levitation] \todo{original project using git scm for blaming, my fork word level}
\end{description}

\subsection{Analysis projects}\label{sub:analysisprojects}

\todo{This section describes useful projects, exemplary  on how to combine different data sources...}

\begin{description}
\item[WikiPride] Python web-application\footurl{https://github.com/declerambaul/WikiPride}{2011}{12}{11}  to visualize contributions of groups of editors that registered in the same month.\footnote{Project website: \weburl{http://meta.wikimedia.org/wiki/Research:WikiPride}{2011}{12}{11}}
\item[Wiki Trip] JavaScript application\footurl{https://github.com/volpino/wikipedia-timeline}{2011}{12}{11} that uses the Wikipedia API as well as a Toolserver backend, see \nameref{sub:toolserver}, to visualize the evolution of a single article over time including: anonymous vs. registered contributors, male vs. female registered users, anonymous edits by country.\footnote{Live demo: \weburl{http://sonetlab.fbk.eu/wikitrip/}{2011}{12}{11}} 
\end{description}


\section{Application design}

\todo{following wikitrip design of a browser application, based on the following technologies}

\subsection{Technologies}

\subsection{Models}

\subsection{Views}

\subsection{Operation}

\begin{todos}
    \item fetch article, wikitext and HTML
    \item parse for dates and location (delegate to subpage) and evaluate article relevancy
    \item fetch authors (toolserver)
    \item fetch revisions
    \item fetch first revision of each language
    \item fetch page views
    \item locate users
    \item fetch annotated source from WikiTrust for selected revisions 
    \item render/store results
\end{todos}

\todo{group mode}

\begin{todos}
    \item fetch article list
    \item analyze all articles
    \item render group results
\end{todos}


\section{Algorithms}

\todo{intro, complementary HTML and wikitext, HTML was easier offering annotations like dtstart}

\subsection{Article requirements}

\todo{list requirements}

\subsection{Date parsing}

\todo{link to Template}

% Tricky http://en.wikipedia.org/wiki/2007_Georgian_demonstrations
% The demonstrations peaked on November 2, 2007, ...

\subsection{Location parsing}

\todo{link to Template}

\subsection{Collective authorship}

\todo{Introduce types of authors (roles) as well as methods to determine contribution/attribution}

\begin{todos}
    \item Autoren
    \item Bots
    \item text attribution, using wikitrust annotations, not for all revisions
\end{todos}

\todo{Are all edits relevant? Edit wars? Bots?}


\subsection{Resolving user names to IPs}

\begin{todos}
    \item registered vs. unregistered vs. bots vs. admins
    \item incorporate key findings of \cite{hardy2011volunteered} as laid out in chapter \ref{sec:georeference}
    \item IPs of unregistered users: Geo lookup
    \item Autoren-Profile: Information Extraction
    \item Geographische Zuordnung vom user profile
\end{todos}

%Zur Bestimmung der Herkunft eines Autors bietet Wikipedia zwei direkte Ansätze: 
%Für jeden Beitrag eines nicht registrierten Benutzers wird die IP-Adresse gespeichert, über die er Zugang zum Internet erlangt hat. 
%Der zweite Ansatz betrifft die registrierten Benutzer.
%Ihre IP-Adressen sind maskiert und nicht öffentlich zugänglich.
%\footnote{Das WikiWatcher-Teilprojekt \emph{Poor Man's Check User} erlaubt eine Auflösung des Benutzernamens in eine IP-Adresse, wenn dieser Nutzer in der Vergangenheit beim Ändern eines Artikels das Session-Limit überschritten hatte. Inzwischen wurde diese Sicherheitslücke in der WikiMedia-Software jedoch behoben. \url{http://wikiwatcher.virgil.gr/pmcu}}
%\footnote{Eine kleine, von der Wikipedia-Community gewählte Nutzerschaft mit der Berechtigung \emph{checkuser} kann die Adressen demaskieren.}
%Die registrierten Nutzer können jedoch auf ihrer \emph{user page} Informationen über ihre Person entweder als Freitext oder strukturiert in \emph{user boxes} veröffentlichen.
%Letztere sind definierte Einheiten mit denen der Nutzer persönliche Eigenschaften wie Herkunftsland, gesprochene Sprachen oder wissenschaftliche Interessen kodifizieren kann.
%Zusammen decken beide Ansätze jedoch nur einen Teil der Beiträge schreibenden Nutzerschaft ab.


\subsection{IP Look-up}

\begin{todos}
    \item Services
    \item Accuracy
    \item Active prevention by proxies and anonymizers: 
    \\ \fullcite{muir2006internet} 
    \\ \fullcite{muir2009internet}
    \\ \fullcite{duckham2005formal}
\end{todos}

%Mit frei verfügbaren\footnote{Die vorgestellten Dienste haben ein tägliches Kontingent an Anfragen. Hilfstechniken wie Caching können diese Einschränkungen jedoch mindern.} Online-Diensten wie \term{Quova}\footnote{\url{http://developer.quova.com}} oder \term{geoplugin}\footnote{\url{http://www.geoplugin.com/webservices}} lässt sich für einen Großteil der IPs daraufhin das Herkunftsland bestimmen.
%
%Im Bezug auf die Herkunft sind sowohl das Land als auch die Geo-Koordinaten interessant.  
%Basierend auf der Versionsgeschichte würde für nicht registrierte Benutzer eine Gewinnung von Daten dann beispielsweise folgende Schritte durchlaufen:
%
%\begin{quotation}
%IP \RA Geolocation-Dienst \RA Koordinaten und Land
%\end{quotation}


\subsection{Parsing user pages}

\begin{todos}
\item Automatic annotation of entities: \url{http://wikipedia-miner.cms.waikato.ac.nz/demos/annotate/}, also has services for categories. Alternative: \url{http://tagme.di.unipi.it/}
    \item IE approach with Machine Learning \fullcite{xiao2004information}
    \item unsupervised IE: \fullcite{etzioni2005unsupervised}
    \item if city is mentioned, determine country (needs disambiguation, e.g. Berlin)
    \item coordinates are optional?
\end{todos}

% IE algorithms based on Templates
%A(723) 755 -> 758 -> 802
%B(132) 143 -> 146 -> 161
%D(85) 91 -> 91 -> 99
%1. Country link part of RegExp, e.g. " comes? from Russia"
%2. Country part of RegExp (no link)
%3. Container around country link contained RegExp. e.g. "and live in a rather small town close to <a>my country's</a> capital"


\subsection{Geographic resolution}\label{sub:resolution}

\begin{todos}
    \item settle for a country
    \item some examples on accuracy for different countries
    \item clustering of origins: areas of influence
\end{todos}


\section{Visualization}

%Auf Basis der strukturierten Daten in Form von Artikeln, Sätzen, Ländern, Koordinaten und Sprachen sollen nun Visualisierungen gefunden werden, welche die Fülle an Informationen zugänglich machen.


\begin{todos}
    \item \fullcite{kjellin2010evaluating}
    \item Identify. Characteristics of an object.
    \item Locate. Absolute or relative position.
    \item Distinguish. Recognize as the same or different.
    \item Categorize. Classify according to some property (e.g., color, position, or shape).
    \item Cluster. Group same or related objects together.
    \item Distribution. Describe the overall pattern.
    \item Rank. Order objects of like types.
    \item Compare. Evaluate different objects with each other.
    \item Associate. Join in a relationship.
    \item Correlate. A direct connection.
\end{todos}

\subsection{Maps}

\begin{todos}
    \item Darstellung der geographischen Analyse
    \item per Wort, Satz, Artikel, Wort
\end{todos}

\subsection{Maps}

%\begin{labeling}{V2}
%\item[V1] Revisionshistogramm à la Google Finance 
%\item[V2] \emph{Heatmap} einer Landkarte mit Ursprüngen der Revisionen 
%\item[V3] Netzwerkgrafik, die Metriken desselben Artikels in verschiedenen Sprachvarianten anzeigt
%\item[V4] Dynamisches Blasendiagramm\footnote{\url{http://en.wikipedia.org/wiki/Motion_chart}} über die Entwicklung unterschiedlicher Sprachvarianten
%\item[V5] \emph{Heatmap} des Artikels mit Stellen höchster Aktivität 
%\item[V6] Landeskürzel für eine gegebene Textstelle
%\item[V7] Edit wars on map, linking two or more places
%\end{labeling}
%

\subsection{Line and scatter charts}


\subsection{Motion chart}


\section{Hypotheses analysis}

\todo{for each hypothesis, what data is gathers, how is it crunched}

\subsection*{H1}

\subsection*{H2}

\subsection*{H3}

\subsection*{H4}

\subsection*{H5}

\subsection*{H6}

\subsection*{H7}

\subsection*{H8}

\subsection*{H9}

\subsection*{H10}

\subsection*{H11}


\section{Possible enhancements}

\subsection{Edit relevance}

\todo{detect reverts, vandalism}

\subsection{Geographic profiling}

\begin{todos}
    \item \fullcite{lieberman2009you}
    \item \fullcite{hecht2010localness}
    \item from other fields such as criminal research: \\ \fullcite{snook2005complexity}
    \item feasibility, maybe just as enhancer
\end{todos}
