%************************************************
\chapter{Apparatus}\label{ch:apparatus}
%************************************************

This chapter describes data sources to get Wikipedia content like articles and revision history as well as tools to retrieve and analyze those.

\todo{Intro aspects of apparatus}

\section{Data sources}\label{sec:datasources}

For an automated analysis, simply browsing Wikipedia's website is not really feasible. 
The bulk of Wikipedia's content like articles, revisions, discussions is stored on its database servers.
Unfortunately, these databases are not directly accessible over the Internet.
The Wikimedia Foundation, however, makes a lot of the data available in the form of database dumps or through an application programming interface (API).

\subsection{Wikipedia website}\label{sub:wpwebsite}

For a complete article analysis, navigating the website can be tedious as one would have to click through a complete revision history and parse the page's source which is formatted using the HyperText Markup Language (HTML).
However, individual pages contain data that is static and can be used throughout the analysis process.
This makes it worth writing a specific parser for a technique known as \emph{screen scraping} to extract the information.
On a high level, it involves the following steps:

\begin{enumerate}
  \item Looking at the HTML source of the page and identifying how the HTML tags and attributes that are used to structure the information.
  \item Writing a parser that addresses the identifying tags and thereby tokenizes the data.
  \item Converting the found tokens into an output format, e.g. JSON.
\end{enumerate}

For a simple HTML table, a parser can be written in a few lines of code.
Using this technique, the following static information was gathered:

\begin{description}
\item[bots] A list of bots was built based on the Wikipedia page \emph{List of bots by number of edits}\footurl{http://en.wikipedia.org/wiki/Wikipedia:List_of_bots_by_number_of_edits}{2012}{01}{24}. 
This list is used to distinguish bots from real authors as contributions done by bots are excluded from the analysis.
There are unregistered bots, however, that appear not in the list.
For a lack of automated distinction, these are counted as normal authors.\footnote{A simple heuristic employed by other software to analyze MediaWiki content is treating all contributors whose username contains or whose comments start with ``bot'' as a bot, e.g. pymwdat, see \weburl{http://code.google.com/p/pymwdat/source/browse/trunk/toolkit.py?spec=svn13&r=13}{2012}{01}{24}. This has a potential for false positives and is not used.}
\item[countries] A list of countries was extracted from the article \emph{ISO\_3166-1}\footurl{http://en.wikipedia.org/wiki/ISO_3166-1}{2012}{01}{2}. 
It provides a list of standardized country names that is also respected by Wikipedia's authors when referring to a country by name.
In a second pass, the Wikipedia article of each country was parsed for coordinates.\footnote{For some countries, coordinates were not present on the page, e.g. \weburl{http://en.wikipedia.org/wiki/Australia}{2012}{01}{24}. In that case, they were manually added by using that country capital's coordinates. For a discussion on geographic resolution, see \nameref{sub:resolution}.}
\end{description}

Making both of these sets static was a design decision recognizing the trade-off between having them in memory and querying for each article.

\subsection{Database dumps}\label{sub:dumps}

Monthly database snapshots of all wikis run by the Wikimedia Foundation, including Wikipedia,  are publicly available\footurl{http://dumps.wikimedia.org}{2011}{12}{11} as database dump files in the XML file format.
For each of the wikis a variety of dumps is available that include all articles and, optionally, their revision history, all categories, interwiki-links, etc.
Despite this openness, some database tables are not publicly available.
The dump files of the database tables \emph{users} and the \emph{watchlist} are kept private.

The dump files can be quite large, e.g. a compressed dump of all articles of the English Wikipedia in their current revision has a size 7.3 GB.\footnote{The uncompressed size is 31.0 GB, see \weburl{http://en.wikipedia.org/wiki/Wikipedia:Database_download}{2011}{12}{11}}
This huge size makes processing them rather slow.\footnote{The project WikiHadoop addresses this problem by offering a stream task format to be used in Hadoop (MapReduce) infrastructure, see \weburl{https://github.com/whym/wikihadoop}{2011}{12}{11}.}
When analyzing only a single article or a category articles, the MediaWiki API can deliver the same information contained in the dumps in a much more targeted manner. 

\subsection{MediaWiki API}\label{sub:mediawikiapi}

Wikipedia runs on the open source software MediaWiki.
This PHP-based wiki package offers a well documented API\footurl{http://www.mediawiki.org/wiki/API:Main_page}{2012}{01}{24} which can be used by other programs to remotely use the wiki's features such as changing content and restoring revisions.\footnote{The full capability of the API can be seen at tried at the \emph{Sandbox} at \weburl{https://en.wikipedia.org/wiki/Special:ApiSandbox}{2012}{01}{24}, a recent addition to the MediaWiki software.}
For analysis of articles, the API offers queries directed at a variety of article properties, e.g. revisions, categories and links.
Among the output formats for the responses is the JavaScript Object Notation (JSON).
Similar to MediaWiki's Special:Export page\footnote{The page \weburl{https://en.wikipedia.org/wiki/Special:Export}{2011}{12}{11} allows for exporting of articles from the English Wikipedia.}, the API also offers an article export that includes all revisions.
\lstset{caption={Example JSON response to a query to list all bots that edited the article \emph{2011-2012 Bahraini uprising}},label=apicall}
\begin{lstlisting}

	{
		"query": {
			"redirects": [{
					"from": "2011 Bahraini uprising",
					"to": "2011-2012 Bahraini uprising"
			}],
			"pages": {
				"30876395": {
					"pageid": 30876395,
					"ns": 0,
					"title": "2011-2012 Bahraini uprising"
				}
			},
			"allusers": [{
					"userid": "13146235", "name": "28bot"
				}, {
					"userid": "5415725", "name": "718 Bot"
				}, ..., {
					"userid": "13770078", "name": "AWBCPBot"
			}]
		},
		"query-continue": {
			"allusers": {
				"aufrom": "AWeenieBot"
			}
		}
	}
\end{lstlisting}

Some of the queries have a limit on how many results they return on a single request.
When there are more results, the response contains a \emph{query-continue} attribute that can be sent with following query so that the next result set can be returned.
The following API calls will be important for this thesis:

\begin{description}
    \item[query info] This basic query is returns essential information like the article ID, the last revision ID, but also the full wikitext of the last revision.
    \item[query revisions] Lists all revisions for an article and for each includes a timestamp and the user as well as the comment for the text change.
    \item[query categorymembers] For a given category, this query lists the articles and subcategories belong to it.
    This query can be used to construct groups of articles for analysis in this thesis.
    \item[query templateembedders] For a given template name, this request lists all pages that embed it.
    This query can also be used to build a group of articles. 
    \item[open search] A method to suggest articles, categories and templates that contain a term. 
    It can be attached to an input field where a user is supposed to enter the name of an article.
    \item[parse] This special query returns the HTML version of the article's wikitext.
    The content that is returned is exactly the HTML source that is sent to a browser when a visitor looks at this article or user page.
    This query will be used in cases where it is easier to parse the HTML markup than the wikitext, e.g. the \nameref{sub:userpages}.
\end{description}

\subsection{Toolserver}\label{sub:toolserver}

The Germany based Wikimedia Deutschland e.V. runs Toolserver\footurl{http://toolserver.org}{2011}{12}{11}, a platform for software tools that can access a continuously updated copy of Wikipedia's databases. 
Among these replicated databases is the English Wikipedia and other major language editions.
However, the deployment of self-made software scripts is restricted and requires an account on Wikimedia's Toolserver.\footnote{I applied for a Toolserver account outlining my necessary database queries, usage profile as well as my affiliation with the Freie Universit\"at Berlin. The application was submitted on 2011-12-21 and has not been processed yet (2012-01-23).}

Some scripts that are already deployed can be accessed freely, allowing them to be reused.
One of these was developed by SoNet\footurl{http://sonetlab.fbk.eu/}{2011}{12}{12}, a social networking research group based in Italy, for a project called WikiTrip (see \nameref{sub:analysisprojects}).
It offers an API\footnote{The API is documented here: \weburl{https://github.com/volpino/toolserver-scripts/tree/master/php}{2011}{12}{12}} to get simple article statistics like article ID, text length as well as complex data structures like a list of unique editors including their gender if they are registered users and chose to reveal their gender in their Wikipedia account.\footnote{Try \weburl{http://toolserver.org/~sonet/api_gender.php?article=Egypt&lang=en}{2011}{12}{11} to get a list of all registered users who edited the article \emph{Egypt} of the English Wikipedia.}

In effect, calling the SoNet API replaces several calls to the original MediaWiki API and therefor speeds up the information retrieval, especially when the number of revisions or authors is high.
The returned data object has the following structure:

\lstset{caption={SoNet API response to a query for the article \emph{2011-2012 Bahraini uprising}},label=sonetapicall}
\begin{lstlisting}
	{
		"first_edit": {"timestamp":1297734917,"user":"Master&Expert"},
		"count":1778,
		"minor_count":401,
		"count_history":{"today":3,"week":5,"month":90,"year":1778},
		"last_edit":1327370324,
		"totaldays":0,
		"average_days_per_edit":"0.00",
		"edits_per_month":0,
		"edits_per_year":0,
		"edits_per_editor":"4.17",
		"editor_count":426,
		"anon_count":337,
		"editors": {"Bahraini Activist":
			{"all":106,"minor":21,"first":"17 May 2011, 09:45:25",
			"last":"22 January 2012, 10:52:50","atbe" 203811,
			"minorpct":"19.81", "size":"140.54","urlencoded":"Bahraini_Activist"},
			...
		},
		"anons":{"2011-02-15T08:11:52Z":
			["78.2.29.139","Rovinj Croatia",45.08,13.64],
			...
		}
	}
\end{lstlisting}

This high density of preprocessed information shows the power of the Toolserver and its direct access to the database.
The property \emph{editors} lists all unique authors of an article and their edit count (property \emph{all}).
The second exhaustive collection is under the property \emph{anons}. 
There, all anonymous contributors are listed with their IP address, their geographic region and coordinates.
The geographic lookup is uses\footurl{https://github.com/volpino/toolserver-scripts/blob/master/php/api.php}{2012}{01}{24} the GeoCityLite database from Maxmind (see \nameref{sub:iplookup} for a discussion).

\subsection{Third-party sources/Web services}\label{sub:webservices}

Like the Toolserver scripts in the previous section, other research projects exist that can be reused as data sources.
Depending on the project's goal, a variety of preprocessed data is available:

\begin{description}
\item[Article traffic] Wikipedia user Henrik\footurl{http://en.wikipedia.org/wiki/User:Henrik}{2011}{12}{12} provides a web service that processes Wikipedia's log files\footnote{These are available at \weburl{http://dumps.wikimedia.org/other/pagecounts-raw/}{2011}{12}{12}} to calculate the number page views per article for a given time.
These statistics can be viewed through a browser\footnote{E.g. \weburl{http://stats.grok.se/en/201105/2011_Egyptian_Revolution}{2011}{12}{12}} or queried through an API \footnote{E.g. \weburl{http://stats.grok.se/json/en/201105/2011_Egyptian_Revolution}{2011}{12}{12}}. 
\item[CatScan] This web service, offered by Toolserver administrator Duesentrieb\footurl{http://meta.wikimedia.org/wiki/User:Duesentrieb}{2011}{12}{13}, finds articles that belong to a given category and its sub-categories (see \nameref{sub:categories} on why this is non-trivial).
It also offers to limit the search to an intersection of categories, e.g. German politicians who are also physicists\footurl{https://toolserver.org/~daniel/WikiSense/CategoryIntersect.php?wikilang=de&wikifam=.wikipedia.org&basecat=Politiker+(Deutschland)&basedeep=3&mode=cs&tagcat=Physiker&tagdeep=2}{2011}{12}{13}.
The results are presented in the browser or can be downloaded as a file containing comma-separated values (CSV format). 
\item[Poor man's checkuser] The project \emph{Poor Man's Check User}\footnote{Project website: \weburl{http://wikiwatcher.virgil.gr/pmcu}{2012}{01}{2}. The project's name is a reference to the \emph{checkuser} permission that a community-elected group of registered users possesses. It allows a de-masking of the IP addresses for each of a registered user's edit.} mapped registered users to IP addresses based on a bug in the session management of the MediaWiki software.\footnote{When a user exceeded a certain time while editing an article without submitting the current changes, the user's session expired on the server. When the edit was submitted after the expiration the user appeared as an anonymous author, being only known by his IP address. When the user then logged in again, the same change was sent again. Scanning all revisions for the same change set therefor allowed for a matching between user name and IP address. This loophole has been closed, however.} 
For the period the bug has been active, some usernames could be mapped.
Naturally, the more edits a user did in this period, the more likely is an appearance in this list.
For the purpose of this thesis, I screen-scraped the entire table and condensed\footnote{Some usernames have multiple entries as each occurrence of the bug created a unique ``evidence''. Among those, some have been manually verified and ranked. When multiple entries exist, my algorithm picks the top ranked.} it to 14,171 unique users.
\item[Quova] This geo-location web service maps an IP address to a geographic location, see \nameref{sec:georeference}.
\item[WikiTrust] Based on \textcite{adler2008assigning} an open source online reputation system\footurl{http://www.wikitrust.net/vandalism-api}{2011}{10}{31} was set up by the University of California, Santa Cruz, to allow for easy vandalism detection (see \nameref{sec:contribution}).
Given an article ID and a revision ID, the API method \emph{wikimarkup} returns an annotated version of the wikitext of that revision.
An annotation consists of a trust value, the revision ID the text got introduced into the article as well as the authors user name or IP address, e.g. revision 473029564 of the article \emph{2011-2012 Bahraini uprising}\footurl{http://en.collaborativetrust.com/WikiTrust/RemoteAPI?method=wikimarkup&pageid=30876395&revid=473029564}{2012}{01}{23}:
\lstset{language=HTML,caption={Excerpt of the annotated markup for the revision \\473029564 of the article \emph{2011-2012 Bahraini uprising}},label=wikitrustapicall}
\begin{lstlisting}
	{{#t:7,468889105,Kudzu1}}The 
	{{#t:7,470041169,Happysailor}}2011-2012 
	{{#t:8,413989516,Master&Expert}}Bahraini 
	{{#t:8,427545590,Kudzu1}}uprising, sometimes called the
	{{#t:9,455029613,Sitrawi86}}February 14 Revolution 
\end{lstlisting}
All wikitext following an annotation, up to the next one, was written by that author.
The web service provider implemented a custom diff algorithm for the attribution of authorship.
This was needed to overcome wiki-specific issues and to maximize tracking, e.g. for text that is removed and re-inserted at a later revision.\footurl{http://www.wikitrust.net/frequently-asked-questions-faq\#TOC-On-text-author-and-origin}{2012}{01}{24}
\end{description}


\section{Available Tools}

To process the data from all the data sources, a wide range of software tools are available in the open source community.
A simple search for ``Wikipedia'' on GitHub\footurl{https://github.com/search?q=wikipedia&type=Repositories}{2011}{12}{12}, a source code exchange platform, shows a multitude of small software projects.
These come in different programming languages and different feature sets and usually help in downloading articles in batches and extract data from big dump files. 
Developed by vigilantes and researchers alike, these programs facilitate both data retrieval and processing.

\subsection{Toolkits}\label{sub:toolkits}

A group of openly available software packages\footnote{Although none of these were used for the content analysis of this thesis, their study proved very insightful on how to process Wikipedia's content.} qualify as swiss-army knifes for processing and analyzing \nameref{sub:dumps}:

\begin{description}
\item[pywikipedia] As the mother of all Python toolkits, the Python wikipedia robot framework\footurl{http://pywikipediabot.sourceforge.net/}{2012}{01}{24} offers an extendable set of classes for all MediaWiki entities like a page, a user, revision, etc, and is typically used to write a bot program for automated editing tasks (see \nameref{sub:authors}).
\item[pymwdat] Based on \entity{pywikipedia}, this toolkit offers a convenient downloader for all revisions of an article as well as an extensible dump file analyzer with support for filtering and revert detection.
\item[levitation] A creative project to turn \nameref{sub:dumps} into a Git\footurl{http://git-scm.com/}{2012}{01}{24} repository.
As a source code management system, Git offers a more space efficient way to store the sequence of revisions of the articles, since it only stores the difference between revisions.
Once converted to a repository, moving from revision to revision is much faster than processing the large dump files.
Git's diff mechanism together with its \emph{blame} command can be used as an alternative way to attribute authorship to passages of article content.\footnote{In fact, git operates on a line level, making the attribution rather coarse. To get blaming functionality on a word level, I patched the source, see my fork at \weburl{https://github.com/davkal/levitation/commit/5fca0001d26cb67fde6ff9d8a5f2b1414cf7681e}{2012}{01}{24}.}
\end{description}

\subsection{Analysis projects}\label{sub:analysisprojects}

In addition to the toolkits, a handful of research projects exist that process Wikipedia's content.
These are purpose built applications that have a much narrower focus but are very skillful in combining and using different data sources such as \nameref{sub:mediawikiapi}, the \nameref{sub:toolserver} or other \nameref{sub:webservices}:

\begin{description}
\item[WikiPride] Python web-application\footurl{https://github.com/declerambaul/WikiPride}{2011}{12}{11}  to visualize contributions of groups of editors that registered in the same month.\footnote{Project website: \weburl{http://meta.wikimedia.org/wiki/Research:WikiPride}{2011}{12}{11}}
\item[Wiki Trip] JavaScript application\footurl{https://github.com/volpino/wikipedia-timeline}{2011}{12}{11}, written at SoNet, that uses the \nameref{sub:mediawikiapi} as well as its own \nameref{sub:toolserver} scripts, to visualize the evolution of a single article over time including: anonymous vs. registered contributors, male vs. female registered users, anonymous edits by country.\footnote{Live demo: \weburl{http://sonetlab.fbk.eu/wikitrip/}{2011}{12}{11}} 
\end{description}


\section{Application design}

For the analysis of articles I developed an application that could draw data from the different sources and process them in a timely fashion.
Following the impressive WikiTrip application design (see \nameref{sub:analysisprojects}) I decided to build a web application that runs entirely in a web browser.\footnote{Application development was done mainly using the Google Chrome browser. The application should run in any HTML5-capable browser.}

\subsection{Technologies}

As a web application the software heavily relies on HTML version 5, JavaScript (JS) and Cascading Stylesheets (CSS). 
It uses a range of open-source toolkits and libraries for a variety purposes:

\begin{description}
  \item[Bootstrap] Twitter's web application toolkit\footurl{http://twitter.github.com/bootstrap/}{2012}{01}{25} controls the basic layout, the styling of sections and form fields, and the navigation bar at the top.
  \item[jquery] The JS library jQuery\footurl{http://jquery.com/}{2012}{01}{25} is used to asynchronously retrieve data from the various data sources, dynamically insert elements into the layout as well as parsing screen-scraped webpages by making use of its selectors for addressing elements.
  The \entity{autocomplete}\footurl{http://jqueryui.com/demos/autocomplete/}{2012}{01}{25} widget of jQuery's UI library is used to display article suggestions based on what has been entered into the article search field.
  \item[Underscore] Underscore\footurl{http://documentcloud.github.com/underscore/}{2012}{01}{25} is a ``utility-belt library'' for functional programming in JS. 
  Its \emph{map} and \emph{groupBy} methods are being heavily used in the analysis of content.
  \item[Backbone] Built upon Underscore, Backbone\footurl{http://documentcloud.github.com/backbone/}{2012}{01}{25} provides a way to structure a JS application.
  All models, collections and views in the application are encapsulated by Backbone objects that can communicate with each other via events.
  \item[datejs] Date.js\footurl{http://www.datejs.com/}{2012}{01}{25} is a library that was being used to parse dates in articles about events.
  \item[lz77] A JS implementation\footurl{https://github.com/olle/lz77-kit}{2012}{01}{25} of the LZ77 text compression algorithm\footurl{http://en.wikipedia.org/wiki/LZ77_and_LZ78}{2012}{01}{25}. 
  It is used to shrink the size of results when stored in the browser's limited data store\footnote{See \weburl{http://dev.w3.org/html5/webstorage/}{2012}{01}{25} for the limitations.}.
  \item[d3.js] Data-driven documents (d3)\footurl{http://mbostock.github.com/d3/}{2012}{01}{25} is a library to visualize big data sets. 
  This application uses d3's box plots\footurl{http://mbostock.github.com/d3/ex/box.html}{2012}{01}{25} to summarize the quantitive distribution of analysis results.
  \item[Google Chart Tools] Google's\footnote{Although not open source, some of the charts are developed by the open-source community.} chart tools\footurl{http://code.google.com/apis/chart/interactive/docs/index.html}{2012}{01}{25} provide a wide range of chart types that I used in the application, including line chart, scatter chart and motion chart.
\end{description}

\subsection{Models}

Looking at Wikipedia's website as a system, visible items like pages, authors and revisions can be abstracted into classes of objects that can be modeled using \entity{Backbone} (for a complete overview of  all models, see \todo{model chart in appendix}).

When a model is instantiated, it knows where to retrieve the data that will populate its attributes, e.g. a revision collection knows that it can get all revisions of an article's history using the query \emph{revisions} from the  \nameref{sub:mediawikiapi}.
Once the data is retrieved, the attributes of each individual revision object are set which in turn triggers an event, telling a single revision model fetch the annotated wikitext from the WikiTrust API (see \nameref{sub:webservices}).
While some events trigger the retrieval of more detailed information, others indicate that a model's data is fully populated ready for display in the application.

\subsection{Views}

The rendering of models is encapsulated in views.
Following the publish-subscribe pattern, a view listens to changes in a certain model.
When a change event is observed, the view renders itself.
For example, the \emph{article view} is rendered multiple times because it draws data from different sources (the page ID is available sooner than, say, the first sentence of the article). 
Most views, however, rely on several models as they analyze various aspects of an article and then display the results in a chart (see \nameref{sec:visualization}).

\subsection{Main routine}

This section describes an application run on a high level, for noteworthy algorithms for the sub-routines, see \nameref{sec:algorithms}.

When loaded in a browser, the application expects an article title, a category or a template name as input.
In the case of an article being entered, a click on ``Analyze!'' starts following routine, querying the various \nameref{sec:datasources}:

\begin{enumerate}
	\item \entity{query info} is being called to check if the article title is a valid title. 
	A successful query also returns the wikitext of the article's latest revision.
	\item A call to \entity{parse} retrieves the HTML version of the latest revision as well as the article's links to other language editions. 
	Both the wikitext and the HTML are then being parsed for location and dates.
	\item The SoNet Toolserver script is called to retrieve all authors of the article.
	\item For all registered usernames in the author collection, a sub-routine is started to locate the users. 
	\item \entity{query revisions} is being called to retrieve all revisions (excluding wikitext). 
	\item The a subset of the revisions, the annotated wikitext is loaded using WikiTrust API.
	\item For each day of the article's existence, the page view statistics are loaded.
	\item For each language present in the language link collection, the first revision is loaded.
	\item All loaded data is analyzed and the results are stored and rendered in the browser.
\end{enumerate}

Depending on the article's number of revisions, this process takes around one minute and involves 100--500 API calls, most of which are done in parallel. 
During this process, data is rendered whenever it has been arrived and processed.
However, when a category or template name has been entered, only the article overview will be rendered while still all of the above steps are being taken.
On a high level, this \emph{group mode} works as follows:

\begin{enumerate}
    \item Fetch a list of articles by calling \entity{query categorymembers} or \entity{query templateembedders}.
    \item Run the analysis routing on all articles.
    \item Compute group results and show them in the browser.
\end{enumerate}


\section{Algorithms}\label{sec:algorithms}

The individual algorithms described mostly deal with extracting data from the various \nameref{sec:datasources}.
To classify an article as treating an event, the article had to be parsed for a location and a date or date interval.
All algorithms will be written in a Python-esque pseudo-code.

\subsection{Article requirements}\label{sub:articlerequirements}

The presence of a date and a location is a direct requirement for an article to qualify for further analysis when the application is in group mode.
All of the following criteria have to be satisfied:

\begin{itemize}
  \item The article has a location in the form of geographic coordinates or by name, e.g. Cairo.
  \item The article has a date, e.g. 9 November 1989, or a date interval, e.g. May 2007 - August 2007.
  \item The article was created after the event started (with a 3 day tolerance) to sort out events that have been scheduled.
  \item The event (start) date is not before 2002 to make sure that Wikipedia was available as a medium. 
  \item The article does not use certain templates or is part of certain categories, e.g. \emph{Category:Living People}.
  This has been included for the purpose of filtering out articles that passed the previous tests but a clearly not an event article. 
  \item The article has at least 100 revisions.
  \item The article has at least 100 unique authors.
  \item At least 25\% of authors can be located.
\end{itemize}

Even when all of the requirements above are met, some articles may not have a full set of results after their analysis.
They will however be included in the computations for which results are present, see \nameref{hypothesesanalysis}.

\subsection{Date parsing}

The way dates are mentioned in articles are as diverse as the people that write them.
And when it comes to date intervals, e.g. May 8--12 2007, even a specialized date parsing library like \entity{date.js} can only be of limited use.
I wrote a custom parser that for a given text returns first occurrence of an interval or a single date, i.e. if no interval could be found, the parsing is repeated for a single date.

Some articles embed info box templates (see \nameref{sub:templates}) that produce annotated markup using the \entity{hcard}\footurl{http://en.wikipedia.org/wiki/HCard}{2012}{01}{25} microformat.
The annotations can be easily addressed with CSS selectors\footnote{This requires the template to be properly programmed. Some templates mistakenly mark start and end of an interval with the same annotation: \verb"dtstart". On other pages the error lies with the authors who misused the date template, e.g. ``Municipal Library Elevator Coup'' happened on 28 January 1908 which was added as  \verb"Start date|1908|28|01" to the info box but unpredictably rendered as April 1, 1908 possibly due to a mixup in the order, see \weburl{http://en.wikipedia.org/wiki/Municipal_Library_Elevator_Coup}{2012}{01}{25}.} and the values conform to the standard for the representation of date and time, ISO 8601\footurl{http://en.wikipedia.org/wiki/ISO_8601}{2012}{01}{25}.
In listing \ref{datecandidates} the first block tries to parse the microformat annotations.
When they are not present, the first info box is checked for a date field.
As a last resort, the first sentence and paragraph are scanned for dates to not filter out eligible articles like \emph{2007 Georgian demonstrations}\footnote{The article does not contain an info box and a date is only mentioned in the second sentence: ``The demonstrations peaked on November 2, 2007,...'' with more dates to follow in the same paragraph. This particular example already shows how idiosyncratic dates can be codified. See \weburl{http://en.wikipedia.org/wiki/2007_Georgian_demonstrations}{2012}{01}{25}.}

\lstset{caption={Date candidates algorithm},label=datecandidates,language=Python,numbers=left}
\begin{lstlisting}
	if 'dtstart' in HTML:	# checking for hcard
		start = datejs.parse(dtstart.text)
		if 'dtend' in HTML:
			end = datejs.parse(dtend.text) # proper interval
		elif 'ongoing' in dtstart.next.text:
			end = today() # ongoing event
		else:
			end = start + 1 # single day event
	elif 'date' in templates.infoboxes[0]:
		start, end = custom.parse(templates.infoboxes[0].date) # wikitext
	if not start:
		start, end = custom.parse(article.first_sentence) # HTML
	if not start:
		start, end = custom.parse(article.first_paragraph) #HTML
\end{lstlisting}

The custom parser then checks for a range of formats including lazy ones like December 14--19, 2008.
Over a long iterative process, I identified the following tokens from which to construct the date patterns as regular expressions:

\lstset{caption={Date tokens},label=datetokens,language=Python,numbers=left}
\begin{lstlisting}
	ords = ['th', 'st', 'nd', 'rd'];
	tokens = {
		M: "({0})".format(MonthNames.join('|')), # months
		D: "(\d{1,2})" + "({0})?".format(ords.join('|')), # day
		Y: "(\d{4})", #year
		T: "(/|-|--|\sto\s|\sand\s)", # interval delimiter
		O: "('*ongoing'*?|'*present'*)", # ongoing event
		F: "From", # ongoing event
		S: "[,\s]*", # whitespace
		P: "\|", # pipe
		A: "([^- --]*)" # other text
	};
\end{lstlisting}

Using these tokens, I produced patterns to match all encountered date formats, e.g. the pattern for December 14--19, 2008 is \verb"MDTDY", or to capture the even less conform ``From 15 October 2011'' (meaning the event is ongoing) the pattern is \verb"FDMY".
The patterns are ranked by accuracy so that ``12 May 2001 - present'' is matched before ``May 2001 - present''.
The coarsest pattern to match is \verb"Y", a single year (4 digits).

\todo{date resolution day, month, year}

\subsection{Location parsing}

Like dates, locations and even coordinates can be codified in numerous ways. 
Most of the coordinate templates used in the info boxes produce annotated markup, thereby making machine-readable.
I still wrote a custom parser for all the cases where the marker (\verb"geo") is not produced.

Some event articles however, do not have coordinates are clearly describing actions at a location, e.g. \emph{Maspero demonstrations}\footurl{http://en.wikipedia.org/wiki/Maspero_demonstrations}{2012}{01}{25}.
From articles like these, location candidates are scraped and then resolved:

\lstset{caption={Article's locate algorithm},label=articlelocation,language=Python,numbers=left}
\begin{lstlisting}
	function locate(article):
		if 'geo' in article.HTML: # checking for machine-readable coords
			location = custom.parse(article.geo.text)
			return
		else: # look for location candidates
			candidates = []
			# all links from the first info box's location field (wikitext)
			candidates.extend(templates.infoboxes[0].location.links)
			# all flags in the first info box 
			candidates.extend(templates.infoboxes[0].flags) #HTML
			# all links from the first paragraph
			candidates.extend(article.first_paragraph.links) # HTML
		if len(candidates) and article.isMainArticle:
			candidates = candidates[:10]
			until location:
				location = locate(retrieve(candidates.pop()))
			return location
\end{lstlisting}

The above mentioned \emph{Maspero demonstrations} article is exemplary for the candidate list mechanism.
Its info box's location field offers three links to articles of a place: Maspiro, Cairo, Egypt.
They are checked until an article with coordinates is found, in this case after the second try, Cairo\footurl{http://en.wikipedia.org/wiki/Cairo}{2012}{01}{25}.

\subsection{Collective authorship}

Most of the authorship processing is being done by SoNet's Toolserver script\footurl{https://github.com/volpino/toolserver-scripts/blob/master/php/api.php}{2012}{01}{25}, see \nameref{sub:toolserver}.
The PHP-script directly queries a live copy of the Wikipedia database for all revisions of the requested article.
For each revision's author, an edit counter is incremented, and if the author was anonymous, the IP is being resolved to a geographic location.
A collection of authors and all resolved locations are then returned in a JSON object. 

From the response, my application then creates an author collection.
If an author is a registered bot, the author is excluded.
From the location list, a second collection is created to manage all author locations for the current article. 

The attribution of text passages to authors is done by the web service WikiTrust, see \nameref{sub:webservices}. 
It returns an annotated markup that can easily be parsed with the following regular expression (JS):

\lstset{caption=,label=wikitrustre,language=HTML,numbers=none}
\begin{lstlisting}
	/{{#t:\d+,\d+,[^}]*}}/g;
\end{lstlisting}

Barring bots, all edits are considered relevant, i.e. reverts or blanking\footnote{The illegitimate removal of all content of an article, see \weburl{http://en.wikipedia.org/wiki/Wikipedia:VANDTYPES\#Types_of_vandalism}{2012}{01}{25}} are not treated in a special way.


\subsection{Locating users}

The location of all anonymous authors has already been determined by SoNet's Toolserver script.
For each of the remaining (registered) users, it is first checked if the user is included in the \emph{Poor man's checkuser} list (see \nameref{sub:webservices}).
In that case, the username can be resolved to an IP address which, in turn, can be resolved to an accurate location, see \nameref{sub:iplookup}.
For all remaining authors, their user page is parsed for a location, see \nameref{sub:parsinguserpages}.


\subsection{IP Look-up}\label{sub:iplookup}

Resolving the location\footnote{Regarding their accuracy and coverage, their website is rather vague, but coverage is at least 99.8\% on a state level, see \weburl{http://www.quova.com/what/}{2012}{01}{25}} for a given IP address is a simple call to Quova's IP-lookup API.\footnote{For the API call, a registered account is needed. This provides a secret key which has to be used to sign each request.}
The limits of the non-commercial license --- 2 requests per second and a maximum of 1,000 requests per day --- have been overcome by a caching server proxy.


\subsection{Parsing user pages}\label{sub:parsinguserpages}

A user page is scanned for possible locations by parsing its HTML content.
This is easier than parsing the wikitext, since embedded templates can be inconsistent\footnote{See \weburl{http://en.wikipedia.org/wiki/Category:Nation_of_origin_user_templates}{2012}{01}{25}}.
The parser looks for links with country names and then checks if they appear in a certain context:

\lstset{caption={User page location algorithm},label=userpagelocation,language=Python,numbers=left}
\begin{lstlisting}
	candidates = []
	for link in userpage.links:
		if is_country(link.title):
			candidates.append(link)

	patterns = [" comes? from", " am from", "This user is from", 
		"This user is in", " lives? in", " currently living in"]
		
	for candidate in candidates:
		context = link.parent
		for pattern in patterns:
			if context.match(pattern):
				country = candidate
				break
	return country
\end{lstlisting}

Plainly parsing for locations and flags, as done in the article's location parsing, yields too many false positives as some userpages are flooded with flags of countries the user professes to have visited.
However, looking at the context of where a link is appears helps in parsing prose such as:
\begin{quotation}
 ``...and live in a rather small town close to \\\verb"<a title='Austria'>"my country's\verb"</a>" capital''.
\end{quotation}
from the user page of \emph{Nightstallion}\footurl{http://en.wikipedia.org/wiki/User:Nightstallion}{2012}{01}{25}.

% IE algorithms based on Templates
%A(723) 755 -> 758 -> 802
%B(132) 143 -> 146 -> 161
%D(85) 91 -> 91 -> 99
%1. Country link part of RegExp, e.g. " comes? from Russia"
%2. Country part of RegExp (no link)
%3. Container around country link contained RegExp. e.g. "and live in a rather small town close to <a>my country's</a> capital"

\subsection{Geographic resolution}\label{sub:resolution}

As accurate as the IP location services may appear, adding the results of the user page analysis to the set of located authors means having to settle for a country-level resolution.

Both the article location and the user page algorithms are searching for countries.
The location for a country is looked up in the application's country list (see \entity{countries} in \nameref{sub:wpwebsite}).
There each country name is mapped to the geographic coordinates present in its Wikipedia article.
For bigger countries, the coordinates refer to the location of the capital, e.g. \emph{United States}\footurl{http://en.wikipedia.org/wiki/United_States}{2012}{01}{25} while for smaller ones a central point is denominated, e.g. Bahrain\footurl{http://en.wikipedia.org/wiki/Bahrain}{2012}{01}{25}.

\todo{example calculation, with equip-distant circles, bahrain / europe / US, not so bad}

\todo{locals are people less than 1000 miles or same country}
% c = 0; l = $(".wikitable td:nth-child(5)").each(function(index){c += index ? parseInt($(this).text().replace(/,/g, "")) : 0}).length; c / l // average country size according to http://en.wikipedia.org/wiki/List_of_countries_by_area

\subsection{Signature distance}

In his dissertation, \textcite{hardy2011volunteered} developed a proximity metric called \emph{signature distance}.
This calculates the ``average distance between an article and all its contributing authors, weighted by the relative work
per author''\footnote{See p. 52 for the complete development of the formula. For clarity, this quotation has been stripped of mathematical symbols.}.

His formula for the signature distance is uses basic properties of articles and authors.
Using the same notation, let $\rho$ be an author and $\alpha$ be an article.
For a sample of articles $S$ let $P = \{\rho : \rho \in S\}$ be the set all articles in the sample and $A = \{\alpha : \alpha \in S\}$ be the set of all authors in the sample.
Then, $\eta(\rho,\alpha)$ is the contribution(s) of author $\rho$ to article $\alpha$ and $N(\alpha) = \{\eta(\rho,\alpha) : \rho \in P\}$ are all contribution(s) to article $\alpha$.
Conversely, $P(\alpha) = \{\rho : \rho \in N(\alpha)\}$ are the author(s) who have made contributions to article $\alpha$.
To calculate the relative work, let $w(\rho,\alpha) = |\eta(\rho,\alpha)| \div |N(\alpha)|$ be relative edit frequency for author $\rho$ on article $\alpha$. 
Let further $\delta(\rho,\alpha)$ be the geodesic distance between author $\rho$ and article $\alpha$.
Then, averaging the distances of authors weighted by their relative work leads to the signature distance $D(\alpha)$:
\begin{eqnarray}
D(\alpha) & = & \sum_{\forall \rho \in P(\alpha)} \frac{|\eta(\rho,\alpha)| \cdot \delta(\rho,\alpha)}{|N(\alpha)|} \\
 & = &  \sum_{\forall \rho \in P(\alpha)} \big(w(\rho,\alpha) \cdot \delta(\rho,\alpha)\big)
\end{eqnarray}

Given that an article has a location and authors that have been located, the function 4.2 was implemented as follows:

\lstset{caption={Signature distance algorithm},label=signaturedistance,language=Python,numbers=left}
\begin{lstlisting}
	sd = 0 # signature distance
	total = 0 # number of all edits
	for author in article.authors:
		if loc in author: # count only authors that have a location
			dist = geodesic(author.loc, article.loc)
			edits = author.count # work is the edit count
			total += edits # postpone relativization to last line 
			sd += dist * edits
	return sd / total
\end{lstlisting}

The algorithm in listing \refname{signaturedistance} relies on the preprocessed edit counts by SoNet's Toolserver script and provides the signature distance only for the latest revision. 
The computation of the signature distance for all revision is more expensive.
Given at least two differently located authors the signature distance changes with each new revision that has a locatable author.
That means for each revision, all previously located edits have to be counted.
As the computation for revision $\eta_{n}$ would have to go over the same edits as the computation for $\eta_{n+1}$, the algorithm (see listing \ref{signaturedistanceall}) uses a technique called \emph{memoization} where previous results are stored within the function for later use.

\lstset{caption={Signature distance algorithm for all revisions},label=signaturedistanceall,language=Python,numbers=left}
\begin{lstlisting}
	function compute(i, located):
		revision = located[i]
		dist = geodesic(revision.author.loc, article.loc)
		if i == 0:
			return dist
		return (dist + (i - 1) * compute(i - 1, located)) / i;
		
	memoized = memoize(compute)
	# only revisions that have a located author
	located = revisions.filter_location() 
	sd = 0
	for revision, index in located:
		sd = memoized(index, located)
		revisions.set(sd)
\end{lstlisting}

\section{Visualization}\label{sec:visualization}

\todo{find nicer intro}
Visualizations should support the presentation of results and invite exploration.
The results are in fact measurements, i.e. series of numbers, that can also be represented in graphical terms.

%\begin{todos}
%    \item \fullcite{kjellin2010evaluating}
%    \item Identify. Characteristics of an object.
%    \item Locate. Absolute or relative position.
%    \item Distinguish. Recognize as the same or different.
%    \item Categorize. Classify according to some property (e.g., color, position, or shape).
%    \item Cluster. Group same or related objects together.
%    \item Distribution. Describe the overall pattern.
%    \item Rank. Order objects of like types.
%    \item Compare. Evaluate different objects with each other.
%    \item Associate. Join in a relationship.
%    \item Correlate. A direct connection.
%\end{todos}

\subsection{Maps}

To show the location that was extracted from an article, a simple world map is used.
A marker shows the position of the location's geographic coordinates.

\img{articlelocation}{Article location for \emph{2011--2012 Bahraini uprising}}

The author analysis groups the located authors by country. 
For each country, the number of edits of its authors is counted, resulting in a mapping of country name to edit count.
A choropleth map (see figure \imgref{origins}) is used to show how this measure varies over different countries.
The darker a country is rendered, the higher is its edit count.

\img{origins}{Geographic origins by country for located authors of \emph{2011--2012 Bahraini uprising}}

However, this choropleth map only measures the edit counts and does not acknowledge the fact that contributions may disappear by edits in later revisions. 
Therefor, a second choropleth map (see figure \imgref{survival}) is shown below the first one, displaying text volume  based on text survival (see also \nameref{sec:contribution}).
This allows for a direct comparison of contributor countries with a high activity (edit counts) versus contributor countries whose citizens write text that survives the scrutiny of other editors, e.g. Egypt gained intensity (darker green) in the second map.

\img{survival}{Text survival in revision 471577075 grouped by country for located text of \emph{2011--2012 Bahraini uprising}}

For the second choropleth map, the annotated markup provided by WikiTrust is parsed to extract the authors that introduced the text sequences that make up the final text.
This subset of revisions is again grouped by the country of its author (if located):

\lstset{caption={Edit weight for map},label=editweight,language=Python,numbers=left}
\begin{lstlisting}
	revision = article.revisions.last
	total = revision.length
	countries = {} # country name -> [length of sequence, ...]
	edits = {} # country name -> proportion of whole text
	for author in revision.authors:
		if loc in author: # add only located authors
			if loc.country not in countries:
				countries[country] = []
			countries[country].append(author.text.length)
	for country in countries.keys:
		edits[country] = sum(countries[country]) / total
	return edits
\end{lstlisting}

\subsection{Line charts}

A timeline chart is used to display the evolution of the several metrics over time: the signature distance, the distance of single revision, and the number of page views for that day, see figure \imgref{activity}.
A peak in the orange line (page views) could signal elevated interest in the content while a tight zig-zag pattern in the red line (revision distance) could be an edit war between local and distant contributors.

\imgwide{activity}{Activity chart for \emph{2011--2012 Bahraini uprising}}

\subsection{Motion chart}

The motion chart offers an alternative view on the metrics shown in the choropleth map.
This chart type is ideal to follow the change in several indicators over time.
Each country is represented by a bubble that, depending on the metric chosen, either moves along an axis or changes its size.
The application uses Google's implementation of the motion chart\footurl{http://code.google.com/apis/chart/interactive/docs/gallery/motionchart.html}{2012}{01}{25}.
It is interactive and invites the user to explore the data by allowing to freely choose which metric should be represented by which axis.

\img{evolution}{Temporal development of edit counts by country for \emph{2011--2012 Bahraini uprising}}

Even clearer than the choropleth map, the motion chart identifies the main contributor countries, e.g. the big blue bubble (Bahrain) and the red one (United States) in figure \imgref{evolution}.
In the example, changing the metric on the y-axis to show the proportion of located text reveals that contributions from Pakistan have a higher survival rate (the yellow bubble moved up), see figure \imgref{evolution2}. 

\img{evolution2}{Temporal development of text proportion by country for \emph{2011--2012 Bahraini uprising}}


\section{Hypotheses analysis}\label{hypothesesanalysis}

The data gathered in the article analysis will be used to test the hypotheses.
In addition to the basic requirements (see \nameref{sub:articlerequirements}) for each article to qualify, the following sections describe for each hypothesis which data from the content analysis is used in testing support for it.

For hypotheses that rely on a correlation for their support, the Pearson product-moment correlation coefficient is used.
\todo{outliers}

\subsection*{H1: Articles are created with only a short delay after the start date of the event.}

In case the date parsing algorithm finds a date, an article is assumed to treat an event.
The parsing supports different resolutions however.
Some events have a precise date-to-date interval, e.g. \emph{2011 Dohuk riots}\footurl{http://en.wikipedia.org/wiki/2011_Dohuk_riots}{2012}{01}{27} while others merely happened over the course of a month, e.g. \emph{February 2010 Australian cyberattacks}\footurl{http://en.wikipedia.org/wiki/February_2010_Australian_cyberattacks}{2012}{01}{27} or even a year.

An article qualifies for this hypothesis if the date parsing resulted in dates in a day or month resolution.
The hypothesis is supported when on average articles were created within a certain time after the event's start date.
For an article start date with a day resolution this limit is 7 days, for articles with a month resolution the limit is 30 days.

\subsection*{H2: The more recent an article, the shorter is the delay between the event start and article creation.}

The qualification criteria are the same as for H1. 
For each article the time difference between article creation and event start date is calculated. 
The hypothesis is supported if there is a linear dependence between article creation date and the time difference, in the form of a negative correlation coefficient.

\subsection*{H3: Articles are being created first in the English Wikipedia.}

Only articles qualify that exist in multiple language editions.
Articles lend support to the hypothesis if they have been created first in the English Wikipedia.

\subsection*{H4: Articles about political events are created by people in the events' proximity.}

Articles qualify if their creator has been located.
An article lends support to the hypothesis if the country derived from the creator's location is the same as the country derived from the article location.

\subsection*{H5: In the beginning of the event anonymous users contribute more than registered users.}

Articles qualify that have at least 10 revisions (excluding bots) in the time interval after creation.
The interval for event articles with day resolution is 7 days while the limit for articles with a month resolution is 30 days.
An article lends support to the hypothesis when the number of anonymous contribution is bigger than the number of contributions done by registered users.

\subsection*{H6: For the duration of the event there are more local contributions than distant ones.}

Articles qualify when they have at least 10 revisions (where the author was located, excluding bots) for the duration of the event.
A local contribution is one from the same country. \todo{limit geodesic distance?}
An article lends support to the hypothesis when the number of local contribution is bigger than the number of distant, i.e. non-local, contributions.

\subsection*{H7: Articles of a political event that has ended will continuously shrink in size.}

Articles qualify that have an event end date before the date they were analyzed and have at least 10 revisions after their end date.
The hypothesis is supported if there is a linear dependence between the time passed after the end date and the article size, in the form of a negative correlation coefficient.

\subsection*{H8: After an event has ended, there will be more contributions from registered users than from anonymous ones.}

Articles qualify that have an event end date before the date they were analyzed and at least 10 revisions (excluding bots) in the time after the event has ended.
An article lends support to the hypothesis when the number of anonymous contribution is bigger than the number of contributions done by registered users.

\subsection*{H9: After an event has ended, the spatial distribution of the contributors will become less local.}

\todo{correlation enough?}

\subsection*{H10: For the duration of the event the article text contains more local contributions than distant ones.}


\subsection*{H11: After an event has ended, the spatial distribution of the surviving contributions will become less local.}

\todo{correlation enough?}

\section{Possible enhancements}

\subsection{Edit relevance}

\todo{detect reverts, vandalism}

\subsection{user page parsing}

\begin{todos}
\item Automatic annotation of entities: \url{http://wikipedia-miner.cms.waikato.ac.nz/demos/annotate/}, also has services for categories. Alternative: \url{http://tagme.di.unipi.it/}
    \item if city is mentioned, determine country (needs disambiguation, e.g. Berlin)
\end{todos}

\subsection{Geographic profiling}

\begin{todos}
    \item \fullcite{lieberman2009you}
    \item \fullcite{hecht2010localness}
    \item from other fields such as criminal research: \\ \fullcite{snook2005complexity}
    \item feasibility, maybe just as enhancer
\end{todos}

